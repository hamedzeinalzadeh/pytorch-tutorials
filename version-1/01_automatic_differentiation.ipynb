{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7589ff48-8d59-449c-8bf5-f8fba18f6a21",
   "metadata": {},
   "source": [
    "# **Pytorch Automatic Differentiation**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4fb5c-a799-496e-a466-920cbccaa919",
   "metadata": {},
   "source": [
    "This section is about implementation of Gradient Descent by the commands below:\n",
    "```python \n",
    "torch.autograd.backward()\n",
    "torch.autograd.grad()\n",
    "```\n",
    "Check out the links for more details:\n",
    "\n",
    "https://pytorch.org/docs/stable/autograd.html\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/autograd.html\n",
    "\n",
    "**Note:** Only the input Tensors we create ourselves will not have associated Function objects.\n",
    "\n",
    "The PyTorch autograd package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor's .requires_grad attribute is set to True, it starts to track all operations on it. When an operation finishes you can call .backward() and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its .grad attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d8e66-c24f-4909-af19-6fd30e421ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
